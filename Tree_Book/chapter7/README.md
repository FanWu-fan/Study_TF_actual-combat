# Chapter7 TF实现循环网络
循环神经网络( **RNN** )和 **Word2Vec**,RNN在NLP(Nature Lanauage Processing)最长使用，而 **Word2Vec**则是将语言中的字词转化为计算机可以理解的稠密向量(Dense Vector),进而可以做其他自然语言处理任务。

## 7.1 TF实现Word2Vec
word2vec叫做“词向量”或“词嵌入”。将语言中字词转为向量形式表达(Vector Representations)的模型，图像，音频等数据天然可以编码并存储为稠密向量的形式。图片是像素点的稠密矩阵，音频是声音信号的频谱数据。自然语言处理在word2vec出现之前，通常将字词转成离散的单独的符号，比如将“中国”转为编号5178的特征，“北京”转为编码为3987的特征。这即是 One-Hot Encoder,一个词对应一个向量(向量中只有一个值为1，其余为0)，通常需要将一篇文章中的每一个词都转为一个向量，而整篇文章则变为一个稀疏矩阵。对文本分类模型，我们使用 Bag of Words模型，将文章对应的稀疏矩阵合并为一个向量，即把每一个词对应得向量加到一起，这样只统计每个词出现的次数，比如“中国”出现23次，那么第5178个特征为23，北京出现2次，那么第3987个特征为2.

使用 One-Hot Encoder有一个问题，即我们对特征的编码往往是随机的，没有提供任意关联信息，没有考虑字词间可能存在的关系。同时，将字词存储为稀疏向量的话，需要更多的数据来训练。使用**向量表达**(Vector Representations)则可以有效地解决这个问题。**向量空间模型**(Vector Space Models)可以将字词转为**连续值**(相对于One-Hot Encoder的离散值)的向量表达，并且其中意思相近的词被映射到向量空间中相近的位置。向量空间模型在NLP中主要依赖的假设是 **Distributional Hypothesis**，即在相同语境中出现的词其语义也相近。向量空间模型大致可以分为两类，一类是**计数模型**，一类是**预测模型**。计数模型统计在语料库中，相邻出现的词的频率。再把这些计数统计结果转为小而稠密的矩阵；而预测模型则根据一个词周围相邻的词推测出这个词，以及它的向量空间。

Word2Vec即是一种计算非常高效的，可以从原始语料中学习字词空间向量的预测模型。它主要非为 CBOW 和 Skip-Gram两种模式。
